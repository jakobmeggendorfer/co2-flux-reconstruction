{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils.data_loader import load_as_maps\n",
    "from models.unet import build_unet\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "features, targets = load_as_maps(start_year=1958, end_year=2018, datasets=[\"exp1\"])\n",
    "\n",
    "# split data\n",
    "X_train = features[:int(0.8 * len(features))]\n",
    "Y_train = targets[:int(0.8 * len(targets))]\n",
    "X_val = features[int(0.8 * len(features)):int(0.9 * len(features))]\n",
    "Y_val = targets[int(0.8 * len(targets)):int(0.9 * len(targets))]\n",
    "X_test = features[int(0.9 * len(features)):]\n",
    "Y_test = targets[int(0.9 * len(targets)):]\n",
    "\n",
    "# scale data\n",
    "scaler = MinMaxScaler()\n",
    "n_samples, h, w, n_features = X_train.shape\n",
    "X_train_flat = X_train.reshape(-1,n_features)\n",
    "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
    "X_train = X_train_scaled_flat.reshape(n_samples, h, w, n_features)\n",
    "\n",
    "\n",
    "n_samples, h, w, n_features = X_val.shape\n",
    "X_val_flat = X_val.reshape(-1,n_features)\n",
    "X_val_scaled_flat = scaler.transform(X_val_flat)\n",
    "X_val = X_val_scaled_flat.reshape(n_samples, h, w, n_features)\n",
    "\n",
    "n_samples, h, w, n_features = X_test.shape\n",
    "X_test_flat = X_test.reshape(-1,n_features)\n",
    "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
    "X_test = X_test_scaled_flat.reshape(n_samples, h, w, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:51:56,896] A new study created in memory with name: no-name-8ec02b1f-245c-439b-96c6-4e15a5759dde\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 6s/step - loss: 2.0554 - mae: 0.9131 - val_loss: 89592176.0000 - val_mae: 7220.2891 - learning_rate: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:53:55,197] Trial 0 finished with value: 89592176.0 and parameters: {'batch_size': 2, 'lr': 0.001, 'kernel_size': 7, 'dropout_rate': 0, 'base_filters': 64, 'optimizer': 'nadam'}. Best is trial 0 with value: 89592176.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 7s/step - loss: 3.1316 - mae: 1.2270 - val_loss: 5530622407737344.0000 - val_mae: 58151008.0000 - learning_rate: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:54:32,876] Trial 1 finished with value: 5530622407737344.0 and parameters: {'batch_size': 8, 'lr': 0.002, 'kernel_size': 7, 'dropout_rate': 0, 'base_filters': 32, 'optimizer': 'rmsprop'}. Best is trial 0 with value: 89592176.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - loss: 1.0449 - mae: 0.6818 - val_loss: 8147.1196 - val_mae: 78.8270 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:55:12,494] Trial 2 finished with value: 8147.11962890625 and parameters: {'batch_size': 2, 'lr': 0.0005, 'kernel_size': 7, 'dropout_rate': 0.2, 'base_filters': 32, 'optimizer': 'adam'}. Best is trial 2 with value: 8147.11962890625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4s/step - loss: 1.5651 - mae: 0.8901 - val_loss: 1.1481 - val_mae: 0.6182 - learning_rate: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:55:36,099] Trial 3 finished with value: 1.1480820178985596 and parameters: {'batch_size': 8, 'lr': 0.001, 'kernel_size': 5, 'dropout_rate': 0.1, 'base_filters': 32, 'optimizer': 'nadam'}. Best is trial 3 with value: 1.1480820178985596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 254ms/step - loss: 0.9428 - mae: 0.6288 - val_loss: 1.1235 - val_mae: 0.6235 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:55:49,620] Trial 4 finished with value: 1.1235020160675049 and parameters: {'batch_size': 1, 'lr': 0.0005, 'kernel_size': 3, 'dropout_rate': 0.05, 'base_filters': 32, 'optimizer': 'adamw'}. Best is trial 4 with value: 1.1235020160675049.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 11s/step - loss: 2.7105 - mae: 1.0429 - val_loss: 458055208665088.0000 - val_mae: 7677925.0000 - learning_rate: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:57:47,299] Trial 5 finished with value: 458055208665088.0 and parameters: {'batch_size': 4, 'lr': 0.002, 'kernel_size': 7, 'dropout_rate': 0.2, 'base_filters': 64, 'optimizer': 'adamw'}. Best is trial 4 with value: 1.1235020160675049.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 6s/step - loss: 1.6761 - mae: 0.8889 - val_loss: 42532740.0000 - val_mae: 5083.1450 - learning_rate: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:58:50,383] Trial 6 finished with value: 42532740.0 and parameters: {'batch_size': 4, 'lr': 0.001, 'kernel_size': 5, 'dropout_rate': 0.1, 'base_filters': 64, 'optimizer': 'adam'}. Best is trial 4 with value: 1.1235020160675049.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 994ms/step - loss: 0.8980 - mae: 0.6266 - val_loss: 1.1560 - val_mae: 0.6864 - learning_rate: 2.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:59:33,453] Trial 7 finished with value: 1.1560423374176025 and parameters: {'batch_size': 1, 'lr': 0.0002, 'kernel_size': 7, 'dropout_rate': 0.2, 'base_filters': 32, 'optimizer': 'nadam'}. Best is trial 4 with value: 1.1235020160675049.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - loss: 1.4303 - mae: 0.8228 - val_loss: 1.1331 - val_mae: 0.6260 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 16:59:55,634] Trial 8 finished with value: 1.133068561553955 and parameters: {'batch_size': 4, 'lr': 0.0005, 'kernel_size': 5, 'dropout_rate': 0.2, 'base_filters': 32, 'optimizer': 'rmsprop'}. Best is trial 4 with value: 1.1235020160675049.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 269ms/step - loss: 0.8739 - mae: 0.6170 - val_loss: 1.1110 - val_mae: 0.6356 - learning_rate: 2.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-24 17:00:08,491] Trial 9 finished with value: 1.1109514236450195 and parameters: {'batch_size': 1, 'lr': 0.0002, 'kernel_size': 3, 'dropout_rate': 0.05, 'base_filters': 32, 'optimizer': 'rmsprop'}. Best is trial 9 with value: 1.1109514236450195.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  value (val_loss): 1.1109514236450195\n",
      "  params: {'batch_size': 1, 'lr': 0.0002, 'kernel_size': 3, 'dropout_rate': 0.05, 'base_filters': 32, 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().setLevel('ERROR')  # quieter logs\n",
    "\n",
    "# Optional: determinism\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------- define the objective ----------\n",
    "def build_model(lr, optimizer_name, base_filters, kernel_size, dropout_rate):\n",
    "    \n",
    "    model = build_unet((167, 360, 13),base_filters,kernel_size,dropout_rate)\n",
    "\n",
    "    # ---- choose optimizer ----\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=lr)\n",
    "    elif optimizer_name == \"nadam\":\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    # ----- search space -----\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [1, 2, 4, 8])\n",
    "    lr = trial.suggest_categorical(\"lr\", [0.0001, 0.0002, 0.0005, 0.001])\n",
    "    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 4])\n",
    "    dropout_rate = trial.suggest_categorical(\"dropout_rate\", [0, 0.05, 0.1, 0.2])\n",
    "    base_filters = trial.suggest_categorical(\"base_filters\", [32,64])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\", \"nadam\", \"rmsprop\"])\n",
    "\n",
    "    model = build_model(lr, optimizer_name, base_filters, kernel_size, dropout_rate)\n",
    "\n",
    "    # ----- callbacks (with pruning) -----\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True\n",
    "    )\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7\n",
    "    )\n",
    "    pruning_cb = TFKerasPruningCallback(trial, monitor='val_loss')\n",
    "\n",
    "    # If your data are NumPy arrays X_train/Y_train, X_val/Y_val already exist in scope\n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        epochs=40,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping, lr_scheduler, pruning_cb],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Return the best validation loss from training\n",
    "    val_losses = history.history['val_loss']\n",
    "    return float(np.min(val_losses))\n",
    "\n",
    "# ---------- run the study ----------\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_warmup_steps=10)\n",
    ")\n",
    "# Adjust n_trials to your budget\n",
    "study.optimize(objective, n_trials=100, gc_after_trial=True)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\"  value (val_loss):\", study.best_trial.value)\n",
    "print(\"  params:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "folder_path = \"../../outputs/optuna/u-net\"\n",
    "\n",
    "df = study.trials_dataframe()\n",
    "\n",
    "# sort descending by 'value'\n",
    "df = df.sort_values(by=\"value\", ascending=True)\n",
    "\n",
    "# round the 'value' column to 4 decimals\n",
    "df[\"value\"] = df[\"value\"].round(4)\n",
    "\n",
    "# save to CSV\n",
    "df.to_csv(folder_path + \"/trials_\" + timestamp + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
